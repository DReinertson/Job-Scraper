7/12/18 : Universal scraper
	- As I have been working through my scraper, I've begun to think about efficiency in terms of multiple websites. I know that my code currently works on my website and the only other websites that it would work on are ones that are designed in the exact same way (Which is highly unlikely). 
		**It would be nice if I could figure out a way to make the scraper work on multiple websites with the same code. I could write scraping functions to use per website, i.e. have a function, put the specific URL within that function and all the beautifulSoup functions that scrapes the data and compile at the end, then call original function. 
		**Is there a way to have code figure out what the title of the job is across every website? description of job? location?...etc
	- Still currently trying to figure out how to run the python code through my flash drive so I can work on the projects while away from home. 

7/11/18 : For loop for multiple pages
	- Update:
		* Program now runs even though previous file exists. Not sure why it started working, maybe code was off before and something else was preventing it from completing?
	- Will create for loop at the beginning of the function that will essentially run all of the scraping code for each webpage and then compile everything at the end. 
	- Trying to figure out how many times to iterate for loop. Will locate second to last list item under the Ul with class = pagination. (2nd to last cause last one has the value of "next"). And will create variable equal to that number in second to last li item. 
	- Wrote code that I can't test currently that finds how many 'li' items are within 'ul' and uses the 2nd to last 'li'.get_text()(Used .count() method). This will give the number of pages. Then created Multi_page() that goes through for loop x (number of pages) times appending to job_list with each search page. Will test when home. 

7/10/18 : Writing CSV files
	- Starting to read through the 14.1 csv files: Really just wanting to learn how to save csv file and overwrite current file or save a new version. 
		* https://docs.python.org/3/library/functions.html#open
		* https://docs.python.org/3.4/library/csv.html
	- Problem: 
		*Right now if I run the program, it'll save the file with whatever name I write. If I run program after, with file already in the folder, it won't work. Particularly why I'm reading through documentation. Also, trying to understand how to convert to CSV file and reading through Panda / Beautiful soup. 
		*Looks like right now I'm converting to CSV file using Panda package and not the built-in Python. It does seem like with the built-in python, you can either use open with("file name", 'a') to append data to the old day (may keep compliing and doulbe in size even if things are the same) or I could use open with ("file name", "w") to Overwrite (Which may be the method I want to use, since i'm already organizing by most recent post, I don't care if old data gets overwritten).
		*Panda package seems to have its own way to overwrite:
			data https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html
	-To do :
		* Will try to use panda's documentation to overwrite, if that doesn't work, will go to built-in python csv writing and use that sytanx.

7/9/18 : Sort worked on list. Now to iterate over all pages
	- Sort worked on specific index, didn't need to change to int
	- Used reverse = True argument to put newest posts first. 
	- Converted file to excel file, DataFrame, using beautiful soup. Still need to get a better understanding of how the package works. It sounds like it can work with lists or dicts when transferring data into a DataFrame. Will now find how to gain html link for each job posting and have that as last index of listing. 
	-Facing a problem:
		*Was facing a problem with trying to convert to DataFrame, but it was because the documentation said to put the path of the file that you want to be saved. When I did that, it wouldn't work, but i realized I was already in the directory I wanted to be in, so I just tried making a file name and it worked. 
		*Don't know how to make beautiful soup overwrite file that already exists. will have to convert dates to ints, things aren't ordered the way they should be. 
			*Solution* When I added html link to index, didn't accomodate for it in my sorting function. Switched indices for sorting function and now sorted excel is complete. 
	- Will just need to make for loop function to go through all pages of searched criteria and will be done. 

7/8/18 : Beginning to understand Python Lambda
	- Started reading about Lambda (to use instead of itemgetter. Felt like learning lambda may come in handy as opposed to just using a python package).
	- Made job_sort(list, index), that will sort a list at whatever index you put as the argument. Will check if it works when i get home (computer at work doesn't have python installed). 
	- From my understanding, using lambda is a short-hand for creating a function using DEF. 
	https://www.programiz.com/python-programming/anonymous-function
	- Reading through the site, used map() & filter() as examples. Those functions accept two arguments and the first argument is the lambda function and the 2nd is the list or object.
	- I may need to remove the space from the date within the scraper and turn the potential string into a int to have sort properly work. 
	- Will need to create for loop to go through all pages of search and filter those. Still need to convert this data into an excel or whatever medium I decide upon (Probably excel spreadsheet).
	- Thinking I should either subscripe to flexjobs or I should begin to create a scraper for craig's list or a different site. 
	- Will need to acquire the html for each job and have that in the scraped data so I can access the post from the scraper. 

7/7/18 : Making month strings into respective numbers
	- Thinking about the Web scraper. Had conversation with Eric not fully understanding the point of doing it. He mentioned the idea of using the webscraper one day and finding a way to check if there were new posts between days (i.e. making list that shows only new posts or using the scraper to organize via date posted?)
	- Another thing I was thinking of would be to access the job sites API in order to retrieve entire list of job postings (1000+ per search) and use the job scraper on that instea of doing it page by page. Going to see what the API looks like for flexjobs. 
	- Wondering if it would be more effective to create an object with each variable as a key and use that to create scraped data or put it all into a list. OR! not even making a list or object and using forloop to directly create output. If I were to use Object, wouldn't be able to organize by date posted? If I were to change the month into a number, I could sort properly (If each post was in a list with each variable) based upon the month and post it that way? Maybe even update the list or create another variable to store the sorted version.
	**Made dates into numbers, will create function that sorts the final job_listing variable based upon the date posted and use this for the final excel/webpage post. 
	- Removed html tags from each of the lists. And will use regex to remove spaces from date to sort it and sort job_list at the end using either itemgetter or Lambda (If using lambda, will have to look into documentation of it to understand what it is). 
	- Once I figure out how to sort everything, I will use for loop to go through list of webpages (Each page for the searched criteria) and sort all of this data
	- When sorted I will need to figure out a  way to highlight or show which job posts are new from day to day.
